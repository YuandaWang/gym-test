{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CartPole\n",
    "# Discrete Control input\n",
    "# Actor Critic structure, with tensorflow\n",
    "# using TD-error as advantage\n",
    "# Experience Replay\n",
    "\n",
    "# !!!!! experience replay not works in this way\n",
    "\n",
    "# Hope it works\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "n_observation = 4\n",
    "n_action = 2\n",
    "n_hidden_c = 10\n",
    "n_hidden_a = 10\n",
    "\n",
    "# Critic Network used to estimate state value function v\n",
    "tf_obs_c = tf.placeholder(tf.float32, [None, n_observation])\n",
    "# layers\n",
    "W1_c = tf.Variable(tf.random_normal([n_observation, n_hidden_c], stddev=0.1))\n",
    "b1_c = tf.Variable(tf.random_normal([n_hidden_c]))\n",
    "W2_c = tf.Variable(tf.random_normal([n_hidden_c, 1], stddev=0.1))\n",
    "b2_c = tf.Variable(tf.random_normal([1]))\n",
    "    \n",
    "fc1_c = tf.nn.relu(tf.matmul(tf_obs_c, W1_c) + b1_c)\n",
    "tf_v = tf.matmul(fc1_c, W2_c) + b2_c\n",
    "\n",
    "# training\n",
    "tf_target_v = tf.placeholder(tf.float32, [None, 1])\n",
    "loss_c = tf.reduce_sum(tf.square(tf_target_v - tf_v))\n",
    "trainer_c = tf.train.AdamOptimizer(0.01).minimize(loss_c)\n",
    "\n",
    "# Actor Network\n",
    "tf_obs_a = tf.placeholder(tf.float32, [None, n_observation])\n",
    "tf_action = tf.placeholder(tf.int32, [None, 1])\n",
    "tf_advantage = tf.placeholder(tf.float32)\n",
    "# layers\n",
    "W1_a = tf.Variable(tf.random_normal([n_observation, n_hidden_a], stddev=0.1))\n",
    "b1_a = tf.Variable(tf.random_normal([n_hidden_a]))\n",
    "W2_a = tf.Variable(tf.random_normal([n_hidden_a, n_action], stddev=0.1))\n",
    "b2_a = tf.Variable(tf.random_normal([n_action]))\n",
    "    \n",
    "fc1_a = tf.nn.relu(tf.matmul(tf_obs_a, W1_a) + b1_a)\n",
    "out_a = tf.matmul(fc1_a, W2_a) + b2_a\n",
    "tf_prob = tf.nn.softmax(out_a)\n",
    "tf_prob = tf.squeeze(tf_prob)\n",
    "# training\n",
    "#tf_action_onehot = tf.squeeze(tf.one_hot(tf_action, n_action))\n",
    "#tf_act_prob = tf.reduce_sum(tf_prob * tf_action_onehot, 1)\n",
    "#tf_log_prob = tf.log(tf_act_prob)\n",
    "\n",
    "tf_action_onehot = tf.one_hot(tf_action, n_action)\n",
    "tf_act_prob = tf.reduce_sum(tf_prob * tf_action_onehot)\n",
    "tf_log_prob = tf.log(tf_act_prob)\n",
    "\n",
    "# td_error, advantge as the direction\n",
    "loss_a = -tf_log_prob * tf_advantage\n",
    "trainer_a = tf.train.AdamOptimizer(0.001).minimize(loss_a)\n",
    "\n",
    "# replay buffer\n",
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 10000):\n",
    "        self.buf = []\n",
    "        self.buffer_size = buffer_size * 5\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buf) + len(experience) >= self.buffer_size:\n",
    "            self.buf[0:(len(experience) + len(self.buf))-self.buffer_size] = []\n",
    "        self.buf.extend(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > len(self.buf): \n",
    "            print 'not enough experiences'\n",
    "            batch_size = 1\n",
    "        batch = np.reshape(np.array(random.sample(self.buf, batch_size)), [batch_size, 5])\n",
    "        s_batch = np.vstack(batch[:, 0])\n",
    "        a_batch = np.vstack(batch[:, 1])\n",
    "        r_batch = np.vstack(batch[:, 2])\n",
    "        s1_batch = np.vstack(batch[:, 3])\n",
    "        d_batch = np.vstack(batch[:, 4])\n",
    "        return s_batch, a_batch, r_batch, s1_batch, d_batch\n",
    "    \n",
    "    def dump(self):\n",
    "        self.buf = []\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Init tensorflow\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "env = gym.make('CartPole-v0')\n",
    "gamma = 0.9\n",
    "batch_size = 32\n",
    "pre_train_episodes = 10\n",
    "loss_log = []\n",
    "action_log = []\n",
    "rsum_log = [0]\n",
    "\n",
    "replay_buffer = experience_buffer()\n",
    "\n",
    "for episode in range(1000):\n",
    "    s = env.reset()\n",
    "    rsum = 0\n",
    "    for step in range(200):\n",
    "        if episode > pre_train_episodes:\n",
    "            prob = sess.run(tf_prob, feed_dict={tf_obs_a:[s]})\n",
    "            action = np.random.choice([0, 1], p=prob)\n",
    "        else:\n",
    "            action = np.random.choice([0, 1], p=[0.5, 0.5])\n",
    "        s1, r, d, info = env.step(action)\n",
    "        if d:  r = -20\n",
    "        rsum += r\n",
    "        # save experience\n",
    "        replay_buffer.add(np.reshape(np.array([s, action, r, s1, d]),[1,5]))\n",
    "\n",
    "        # Training\n",
    "        if episode > pre_train_episodes:\n",
    "            # sample experience\n",
    "            s_batch, a_batch, r_batch, s1_batch, d_batch = replay_buffer.sample(batch_size)\n",
    "            v1 = sess.run(tf_v, feed_dict={tf_obs_c:s1_batch})\n",
    "            v = sess.run(tf_v, feed_dict={tf_obs_c:s_batch})\n",
    "            end_edit = -(r_batch - 1)\n",
    "            target_v = r_batch + gamma * v1 * (end_edit)\n",
    "            #if d: target_v = r    later work on this\n",
    "            \n",
    "            # train critic network\n",
    "            for i in range(10):\n",
    "                sess.run(trainer_c, feed_dict={tf_obs_c:s_batch, tf_target_v:target_v})\n",
    "\n",
    "            # train actor network, use td error as advantage\n",
    "            # no batch update\n",
    "            v1_a = sess.run(tf_v, feed_dict={tf_obs_c:[s1]})\n",
    "            v_a = sess.run(tf_v, feed_dict={tf_obs_c:[s]})\n",
    "            target_v_a = r + gamma * v1_a\n",
    "            td_error = target_v_a - v_a\n",
    "            \n",
    "            sess.run(trainer_a, feed_dict={tf_obs_a: [s], \n",
    "                                           tf_action: np.array([[action]]), \n",
    "                                           tf_advantage: td_error})\n",
    "            #td_error = target_v - v\n",
    "            #_, act_prob =  sess.run([trainer_a, tf_act_prob],  feed_dict={tf_obs_a:s_batch, tf_action:a_batch, tf_advantage:td_error})\n",
    "            #print act_prob\n",
    "            \n",
    "        s = s1\n",
    "        \n",
    "        if d: break\n",
    "        \n",
    "    # End of episode\n",
    "    rsum_log.append(rsum * 0.05 + rsum_log[-1] * 0.95)\n",
    "    if episode % 100 == 0:\n",
    "        print 'episode:', episode, 'rsum:', rsum\n",
    "plt.plot(rsum_log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(rsum_log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show\n",
    "for episode in range(200):\n",
    "    s = env.reset()\n",
    "    for step in range(200):\n",
    "        prob = sess.run(tf_prob, feed_dict={tf_obs_a:[s]})[0]\n",
    "        action = np.random.choice([0, 1], p=prob)\n",
    "        s1, r, d, info = env.step(action)\n",
    "        env.render()\n",
    "        s = s1\n",
    "        if d: break\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
